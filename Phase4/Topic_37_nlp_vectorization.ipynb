{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join(os.pardir, os.pardir))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import regexp_tokenize, word_tokenize, RegexpTokenizer\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "import re\n",
    "\n",
    "# Notice that these vectorizers are from `sklearn` and not `nltk`!\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer,\\\n",
    "HashingVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Goals\n",
    "\n",
    "- normalize a lexicon with stemming and lemmatization\n",
    "- run feature engineering algorithms for NLP\n",
    "    - bag-of-Words\n",
    "    - vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = pd.read_csv('data/satire_nosatire.csv')\n",
    "sample_document = corpus.iloc[1].body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = \"([a-zA-Z]+(?:'[a-z]+)?)\"\n",
    "tokenizer = RegexpTokenizer(pattern)\n",
    "sample_doc = tokenizer.tokenize(sample_document)\n",
    "sample_doc = [token.lower() for token in sample_doc]\n",
    "sw = stopwords.words('english')\n",
    "sample_doc = [token for token in sample_doc if token not in sw]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['desperate',\n",
       " 'unwind',\n",
       " 'months',\n",
       " 'nonstop',\n",
       " 'work',\n",
       " 'investigating',\n",
       " 'russian',\n",
       " 'influence',\n",
       " 'election',\n",
       " 'visibly',\n",
       " 'exhausted',\n",
       " 'special',\n",
       " 'counsel',\n",
       " 'robert',\n",
       " 'mueller',\n",
       " 'powered',\n",
       " 'phone',\n",
       " 'friday',\n",
       " 'order',\n",
       " 'give',\n",
       " 'break',\n",
       " 'news',\n",
       " 'concerning',\n",
       " 'probe',\n",
       " 'holiday',\n",
       " 'break',\n",
       " 'last',\n",
       " 'thing',\n",
       " 'want',\n",
       " 'spending',\n",
       " 'time',\n",
       " 'family',\n",
       " 'cascade',\n",
       " 'push',\n",
       " 'notifications',\n",
       " 'telling',\n",
       " 'yet',\n",
       " 'another',\n",
       " 'russian',\n",
       " 'oligarch',\n",
       " 'political',\n",
       " 'operative',\n",
       " 'highly',\n",
       " 'placed',\n",
       " 'socialite',\n",
       " 'used',\n",
       " 'deutsche',\n",
       " 'bank',\n",
       " 'channels',\n",
       " 'funnel',\n",
       " 'money',\n",
       " 'campaign',\n",
       " 'said',\n",
       " 'former',\n",
       " 'fbi',\n",
       " 'director',\n",
       " 'firmly',\n",
       " 'holding',\n",
       " 'phone',\n",
       " 'power',\n",
       " 'button',\n",
       " 'adding',\n",
       " 'wants',\n",
       " 'completely',\n",
       " 'present',\n",
       " 'moment',\n",
       " 'celebrating',\n",
       " 'loved',\n",
       " 'ones',\n",
       " 'ruminating',\n",
       " 'met',\n",
       " 'diplomat',\n",
       " 'whether',\n",
       " 'someone',\n",
       " 'using',\n",
       " 'social',\n",
       " 'media',\n",
       " 'tamper',\n",
       " 'witnesses',\n",
       " 'want',\n",
       " 'two',\n",
       " 'calm',\n",
       " 'weeks',\n",
       " 'even',\n",
       " 'think',\n",
       " 'individual',\n",
       " 'one',\n",
       " 'even',\n",
       " 'say',\n",
       " 'name',\n",
       " 'wait',\n",
       " 'hear',\n",
       " 'important',\n",
       " 'developments',\n",
       " 'january',\n",
       " 'since',\n",
       " 'know',\n",
       " 'second',\n",
       " 'read',\n",
       " 'say',\n",
       " 'something',\n",
       " 'eric',\n",
       " 'involved',\n",
       " 'deeply',\n",
       " 'previously',\n",
       " 'suspected',\n",
       " 'get',\n",
       " 'pulled',\n",
       " 'back',\n",
       " 'ruin',\n",
       " 'whole',\n",
       " 'vacation',\n",
       " 'press',\n",
       " 'time',\n",
       " 'mueller',\n",
       " 'reactivated',\n",
       " 'phone',\n",
       " 'check',\n",
       " 'news',\n",
       " 'real',\n",
       " 'quick']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "111"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(sample_doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming and Lemmatizing\n",
    "\n",
    "#### Stemming\n",
    "Most of the semantic meaning of a word is held in the root, which is usually the beginning of a word.  Conjugations and plurality do not change the semantic meaning. \"eat\", \"eats\", and \"eating\" all have essentially the same meaning. The rest is grammatical variation for the sake of marking things like tense or person or number.   \n",
    "\n",
    "Stemmers consolidate similar words by chopping off the ends of the words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![stemmer](images/stemmer.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are different stemmers available.  The two we will use here are the **Porter** and **Snowball** stemmers.  A main difference between the two is how aggressively it stems, Porter being less aggressive.\n",
    "\n",
    "Retro webpage for Martin Porter [here](https://tartarus.org/martin/PorterStemmer/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_stemmer = nltk.stem.PorterStemmer()\n",
    "s_stemmer = nltk.stem.SnowballStemmer(language=\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'desperate'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_doc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'desper'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_stemmer.stem(sample_doc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'desper'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_stemmer.stem(sample_doc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "highly highli high\n",
      "firmly firmli firm\n"
     ]
    }
   ],
   "source": [
    "for word in sample_doc:\n",
    "    p_word = p_stemmer.stem(word)\n",
    "    s_word = s_stemmer.stem(word)\n",
    "    \n",
    "    if p_word != s_word:\n",
    "        print(word, p_word, s_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_doc_stemmed = [p_stemmer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fdist = FreqDist(sample_doc_stemmed)\n",
    "plt.figure(figsize=(10, 10))\n",
    "fdist.plot(30);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemming slightly reduced our token count: 111 unique tokens\n"
     ]
    }
   ],
   "source": [
    "print(f'Stemming slightly reduced our token count: {len(set(sample_doc))} unique tokens')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatizing\n",
    "\n",
    "Lemmatizing is a bit more sophisticated than the stem choppers. Lemmatizing uses part-of-speech tagging to determine how to transform a word.\n",
    "\n",
    "- Unlike Stemming, Lemmatization reduces the inflected words, properly ensuring that the root word belongs to the language. It can handle words such as \"mouse\", whose plural \"mice\" the stemmers would not lump together with the original. \n",
    "\n",
    "- In Lemmatization, the root word is called the \"lemma\". \n",
    "\n",
    "- A lemma (plural lemmas or lemmata) is the canonical form, dictionary form, or citation form of a set of words.\n",
    "\n",
    "![lemmer](images/lemmer.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = nltk.stem.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/andrewboucher/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Mice\" becomes: mouse\n",
      "\"Media\" becomes: medium\n"
     ]
    }
   ],
   "source": [
    "print(f'\"Mice\" becomes: {lemmatizer.lemmatize(\"mice\")}')\n",
    "print(f'\"Media\" becomes: {lemmatizer.lemmatize(sample_doc[76])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['He', 'saw', 'the', 'tree', 'get', 'sawed', 'down']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# However, look at the output below:\n",
    "    \n",
    "sentence = \"He saw the trees get sawed down\"\n",
    "lemmed_sentence = [lemmatizer.lemmatize(token) for token in sentence.split(' ')]\n",
    "lemmed_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatizers depend, for their full functionality, on POS tagging, and **the default tag is 'noun'**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a little bit of work, we can POS tag our text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Desperate to unwind after months of nonstop work investigating Russian influence in the 2016 election, visibly exhausted Special Counsel Robert Mueller powered his phone down Friday in order to give himself a break from any news concerning the probe over the holiday break. “The last thing I want when I’m spending time with my family is a cascade of push notifications telling me yet another Russian oligarch, political operative, or highly placed socialite used Deutsche Bank channels to funnel money to the campaign,” said the former FBI director, firmly holding down his phone’s power button and adding that he wants to be “completely present in the moment” while celebrating with his loved ones, not ruminating about who met with which diplomat or whether someone was using social media to tamper with his witnesses. “I just want to have two calm weeks where I don’t even think about Individual One. I won’t even say his name. I’ll have to wait to hear about any important developments in January, since I just know the second I read, say, something about Eric being involved more deeply than we previously suspected, I’ll get pulled back in and ruin my whole vacation.” At press time, Mueller had reactivated his phone just to check the news real quick. '"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern = \"([a-zA-Z]+(?:'[a-z]+)?)\"\n",
    "tokenizer = RegexpTokenizer(pattern)\n",
    "sample_doc = tokenizer.tokenize(sample_document)\n",
    "sample_doc = [token.lower() for token in sample_doc]\n",
    "sample_doc = [token for token in sample_doc if token not in sw]\n",
    "corpus.loc[1].body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$: dollar\n",
      "    $ -$ --$ A$ C$ HK$ M$ NZ$ S$ U.S.$ US$\n",
      "'': closing quotation mark\n",
      "    ' ''\n",
      "(: opening parenthesis\n",
      "    ( [ {\n",
      "): closing parenthesis\n",
      "    ) ] }\n",
      ",: comma\n",
      "    ,\n",
      "--: dash\n",
      "    --\n",
      ".: sentence terminator\n",
      "    . ! ?\n",
      ":: colon or ellipsis\n",
      "    : ; ...\n",
      "CC: conjunction, coordinating\n",
      "    & 'n and both but either et for less minus neither nor or plus so\n",
      "    therefore times v. versus vs. whether yet\n",
      "CD: numeral, cardinal\n",
      "    mid-1890 nine-thirty forty-two one-tenth ten million 0.5 one forty-\n",
      "    seven 1987 twenty '79 zero two 78-degrees eighty-four IX '60s .025\n",
      "    fifteen 271,124 dozen quintillion DM2,000 ...\n",
      "DT: determiner\n",
      "    all an another any both del each either every half la many much nary\n",
      "    neither no some such that the them these this those\n",
      "EX: existential there\n",
      "    there\n",
      "FW: foreign word\n",
      "    gemeinschaft hund ich jeux habeas Haementeria Herr K'ang-si vous\n",
      "    lutihaw alai je jour objets salutaris fille quibusdam pas trop Monte\n",
      "    terram fiche oui corporis ...\n",
      "IN: preposition or conjunction, subordinating\n",
      "    astride among uppon whether out inside pro despite on by throughout\n",
      "    below within for towards near behind atop around if like until below\n",
      "    next into if beside ...\n",
      "JJ: adjective or numeral, ordinal\n",
      "    third ill-mannered pre-war regrettable oiled calamitous first separable\n",
      "    ectoplasmic battery-powered participatory fourth still-to-be-named\n",
      "    multilingual multi-disciplinary ...\n",
      "JJR: adjective, comparative\n",
      "    bleaker braver breezier briefer brighter brisker broader bumper busier\n",
      "    calmer cheaper choosier cleaner clearer closer colder commoner costlier\n",
      "    cozier creamier crunchier cuter ...\n",
      "JJS: adjective, superlative\n",
      "    calmest cheapest choicest classiest cleanest clearest closest commonest\n",
      "    corniest costliest crassest creepiest crudest cutest darkest deadliest\n",
      "    dearest deepest densest dinkiest ...\n",
      "LS: list item marker\n",
      "    A A. B B. C C. D E F First G H I J K One SP-44001 SP-44002 SP-44005\n",
      "    SP-44007 Second Third Three Two * a b c d first five four one six three\n",
      "    two\n",
      "MD: modal auxiliary\n",
      "    can cannot could couldn't dare may might must need ought shall should\n",
      "    shouldn't will would\n",
      "NN: noun, common, singular or mass\n",
      "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
      "    investment slide humour falloff slick wind hyena override subhumanity\n",
      "    machinist ...\n",
      "NNP: noun, proper, singular\n",
      "    Motown Venneboerger Czestochwa Ranzer Conchita Trumplane Christos\n",
      "    Oceanside Escobar Kreisler Sawyer Cougar Yvette Ervin ODI Darryl CTCA\n",
      "    Shannon A.K.C. Meltex Liverpool ...\n",
      "NNPS: noun, proper, plural\n",
      "    Americans Americas Amharas Amityvilles Amusements Anarcho-Syndicalists\n",
      "    Andalusians Andes Andruses Angels Animals Anthony Antilles Antiques\n",
      "    Apache Apaches Apocrypha ...\n",
      "NNS: noun, common, plural\n",
      "    undergraduates scotches bric-a-brac products bodyguards facets coasts\n",
      "    divestitures storehouses designs clubs fragrances averages\n",
      "    subjectivists apprehensions muses factory-jobs ...\n",
      "PDT: pre-determiner\n",
      "    all both half many quite such sure this\n",
      "POS: genitive marker\n",
      "    ' 's\n",
      "PRP: pronoun, personal\n",
      "    hers herself him himself hisself it itself me myself one oneself ours\n",
      "    ourselves ownself self she thee theirs them themselves they thou thy us\n",
      "PRP$: pronoun, possessive\n",
      "    her his mine my our ours their thy your\n",
      "RB: adverb\n",
      "    occasionally unabatingly maddeningly adventurously professedly\n",
      "    stirringly prominently technologically magisterially predominately\n",
      "    swiftly fiscally pitilessly ...\n",
      "RBR: adverb, comparative\n",
      "    further gloomier grander graver greater grimmer harder harsher\n",
      "    healthier heavier higher however larger later leaner lengthier less-\n",
      "    perfectly lesser lonelier longer louder lower more ...\n",
      "RBS: adverb, superlative\n",
      "    best biggest bluntest earliest farthest first furthest hardest\n",
      "    heartiest highest largest least less most nearest second tightest worst\n",
      "RP: particle\n",
      "    aboard about across along apart around aside at away back before behind\n",
      "    by crop down ever fast for forth from go high i.e. in into just later\n",
      "    low more off on open out over per pie raising start teeth that through\n",
      "    under unto up up-pp upon whole with you\n",
      "SYM: symbol\n",
      "    % & ' '' ''. ) ). * + ,. < = > @ A[fj] U.S U.S.S.R * ** ***\n",
      "TO: \"to\" as preposition or infinitive marker\n",
      "    to\n",
      "UH: interjection\n",
      "    Goodbye Goody Gosh Wow Jeepers Jee-sus Hubba Hey Kee-reist Oops amen\n",
      "    huh howdy uh dammit whammo shucks heck anyways whodunnit honey golly\n",
      "    man baby diddle hush sonuvabitch ...\n",
      "VB: verb, base form\n",
      "    ask assemble assess assign assume atone attention avoid bake balkanize\n",
      "    bank begin behold believe bend benefit bevel beware bless boil bomb\n",
      "    boost brace break bring broil brush build ...\n",
      "VBD: verb, past tense\n",
      "    dipped pleaded swiped regummed soaked tidied convened halted registered\n",
      "    cushioned exacted snubbed strode aimed adopted belied figgered\n",
      "    speculated wore appreciated contemplated ...\n",
      "VBG: verb, present participle or gerund\n",
      "    telegraphing stirring focusing angering judging stalling lactating\n",
      "    hankerin' alleging veering capping approaching traveling besieging\n",
      "    encrypting interrupting erasing wincing ...\n",
      "VBN: verb, past participle\n",
      "    multihulled dilapidated aerosolized chaired languished panelized used\n",
      "    experimented flourished imitated reunifed factored condensed sheared\n",
      "    unsettled primed dubbed desired ...\n",
      "VBP: verb, present tense, not 3rd person singular\n",
      "    predominate wrap resort sue twist spill cure lengthen brush terminate\n",
      "    appear tend stray glisten obtain comprise detest tease attract\n",
      "    emphasize mold postpone sever return wag ...\n",
      "VBZ: verb, present tense, 3rd person singular\n",
      "    bases reconstructs marks mixes displeases seals carps weaves snatches\n",
      "    slumps stretches authorizes smolders pictures emerges stockpiles\n",
      "    seduces fizzes uses bolsters slaps speaks pleads ...\n",
      "WDT: WH-determiner\n",
      "    that what whatever which whichever\n",
      "WP: WH-pronoun\n",
      "    that what whatever whatsoever which who whom whosoever\n",
      "WP$: WH-pronoun, possessive\n",
      "    whose\n",
      "WRB: Wh-adverb\n",
      "    how however whence whenever where whereby whereever wherein whereof why\n",
      "``: opening quotation mark\n",
      "    ` ``\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package tagsets to\n",
      "[nltk_data]     /Users/andrewboucher/nltk_data...\n",
      "[nltk_data]   Unzipping help/tagsets.zip.\n"
     ]
    }
   ],
   "source": [
    "nltk.download('tagsets')\n",
    "nltk.help.upenn_tagset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use nltk's pos_tag to tag our words\n",
    "# Does a pretty good job, but does make some mistakes\n",
    "\n",
    "sample_doc_tagged = pos_tag\n",
    "sample_doc_tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then transform the tags into the tags of our lemmatizers\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    '''\n",
    "    Translate nltk POS to wordnet tags\n",
    "    '''\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_doc_tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_doc_tagged = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_doc_tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_doc_lemmed = [lemmatizer.lemmatize(token[0], token[1]) for token in sample_doc_tagged]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_doc[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_doc_lemmed[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'There are {len(set(sample_doc_lemmed))} unique lemmas.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist = FreqDist(sample_doc_lemmed)\n",
    "plt.figure(figsize=(10, 10))\n",
    "fdist.plot(30);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3. Feature Engineering for NLP\n",
    "\n",
    "The machine learning algorithms we have encountered so far represent features as the variables that take on different value for each observation. For example, we represent individuals with distinct education levels, incomes, and such. However, in NLP, features are represented in a very different way. In order to pass text data to machine learning algorithms and perform classification, we need to represent the features in a sensible way. One such method is called **Bag-of-words (BoW)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A bag-of-words model, or BoW for short, is a way of extracting features from text for use in modeling. A bag-of-words is a representation of text that describes the occurrence of words within a document. It involves two things:\n",
    "\n",
    "- A vocabulary of known words.\n",
    "- A measure of the presence of known words.\n",
    "\n",
    "It is called a “bag” of words **because any information about the order or structure of words in the document is discarded**. The model is only concerned with whether known words occur in the document, not with **where** they may occur in the document. The intuition behind BoW is that a document is similar to another if they have similar contents. The Bag of Words method can be represented as a **Document Term Matrix**, in which each column is a unique vocabulary n-gram and each observation is a document. Consider, for example, the following **corpus** of documents:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Document 1: \"I love dogs.\"\n",
    "- Document 2: \"I love cats.\"\n",
    "- Document 3: \"I love all animals.\"\n",
    "- Document 4: \"I hate dogs.\"\n",
    "\n",
    "This corpus can be represented as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\downarrow$Doc\\|Word$\\rightarrow$|I|love|dogs|cats|all|animals|hate\n",
    "-|-|-|-|-|-|-|-\n",
    "Document_1|1|1|1|0|0|0|0\n",
    "Document_2|1|1|0|1|0|0|0\n",
    "Document_3|1|1|0|0|1|1|0\n",
    "Document_4|1|0|1|0|0|0|1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorization\n",
    "\n",
    "In order to get these tokens from our documents, we're going to use tools called \"vectorizers\".\n",
    "\n",
    "The most straightforward vectorizer in `sklearn.feature_extraction.text` is the `CountVectorizer`, which will simply count the number of each word type in each document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `CountVectorizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>adding</th>\n",
       "      <th>another</th>\n",
       "      <th>back</th>\n",
       "      <th>bank</th>\n",
       "      <th>break</th>\n",
       "      <th>button</th>\n",
       "      <th>calm</th>\n",
       "      <th>campaign</th>\n",
       "      <th>cascade</th>\n",
       "      <th>celebrating</th>\n",
       "      <th>...</th>\n",
       "      <th>visibly</th>\n",
       "      <th>wait</th>\n",
       "      <th>want</th>\n",
       "      <th>wants</th>\n",
       "      <th>weeks</th>\n",
       "      <th>whether</th>\n",
       "      <th>whole</th>\n",
       "      <th>witnesses</th>\n",
       "      <th>work</th>\n",
       "      <th>yet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 111 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   adding  another  back  bank  break  button  calm  campaign  cascade  \\\n",
       "0       1        1     1     1      2       1     1         1        1   \n",
       "\n",
       "   celebrating  ...  visibly  wait  want  wants  weeks  whether  whole  \\\n",
       "0            1  ...        1     1     2      1      1        1      1   \n",
       "\n",
       "   witnesses  work  yet  \n",
       "0          1     1    1  \n",
       "\n",
       "[1 rows x 111 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# implementing it in python\n",
    "\n",
    "# Convert a collection of text documents to a matrix of token counts\n",
    "\n",
    "vec = CountVectorizer(token_pattern=r\"([a-zA-Z]+(?:'[a-z]+)?)\", stop_words=sw)\n",
    "X = vec.fit_transform(corpus.body[1:2])\n",
    "\n",
    "\n",
    "df = pd.DataFrame(X.toarray(), columns=vec.get_feature_names())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is not very exciting for one document. The idea is to make a document term matrix for all of the words in our corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Noting that the resignation of James Mattis as...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Desperate to unwind after months of nonstop wo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Nearly halfway through his presidential term, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Attempting to make amends for gross abuses of ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Decrying the Senate’s resolution blaming the c...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>Britain’s opposition leader Jeremy Corbyn wou...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>Turkey will take over the fight against Islam...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>Malaysia is seeking $7.5 billion in reparatio...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>An Israeli court sentenced a Palestinian to 1...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>At least 22 people have died due to landslide...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  body  target\n",
       "0    Noting that the resignation of James Mattis as...       1\n",
       "1    Desperate to unwind after months of nonstop wo...       1\n",
       "2    Nearly halfway through his presidential term, ...       1\n",
       "3    Attempting to make amends for gross abuses of ...       1\n",
       "4    Decrying the Senate’s resolution blaming the c...       1\n",
       "..                                                 ...     ...\n",
       "995   Britain’s opposition leader Jeremy Corbyn wou...       0\n",
       "996   Turkey will take over the fight against Islam...       0\n",
       "997   Malaysia is seeking $7.5 billion in reparatio...       0\n",
       "998   An Israeli court sentenced a Palestinian to 1...       0\n",
       "999   At least 22 people have died due to landslide...       0\n",
       "\n",
       "[1000 rows x 2 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>adding</th>\n",
       "      <th>another</th>\n",
       "      <th>back</th>\n",
       "      <th>bank</th>\n",
       "      <th>break</th>\n",
       "      <th>button</th>\n",
       "      <th>calm</th>\n",
       "      <th>campaign</th>\n",
       "      <th>cascade</th>\n",
       "      <th>celebrating</th>\n",
       "      <th>...</th>\n",
       "      <th>visibly</th>\n",
       "      <th>wait</th>\n",
       "      <th>want</th>\n",
       "      <th>wants</th>\n",
       "      <th>weeks</th>\n",
       "      <th>whether</th>\n",
       "      <th>whole</th>\n",
       "      <th>witnesses</th>\n",
       "      <th>work</th>\n",
       "      <th>yet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 131 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   adding  another  back  bank  break  button  calm  campaign  cascade  \\\n",
       "0       1        1     1     1      2       1     1         1        1   \n",
       "1       0        0     1     0      0       0     0         0        0   \n",
       "\n",
       "   celebrating  ...  visibly  wait  want  wants  weeks  whether  whole  \\\n",
       "0            1  ...        1     1     2      1      1        1      1   \n",
       "1            0  ...        0     0     0      0      0        0      0   \n",
       "\n",
       "   witnesses  work  yet  \n",
       "0          1     1    1  \n",
       "1          0     0    0  \n",
       "\n",
       "[2 rows x 131 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec = CountVectorizer(token_pattern=r\"([a-zA-Z]+(?:'[a-z]+)?)\", stop_words=sw)\n",
    "X = vec.fit_transform(corpus.body[1:3])\n",
    "\n",
    "df = pd.DataFrame(X.toarray(), columns=vec.get_feature_names())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>adding</th>\n",
       "      <th>adding wants</th>\n",
       "      <th>administration</th>\n",
       "      <th>administration official</th>\n",
       "      <th>advisor</th>\n",
       "      <th>advisor michael</th>\n",
       "      <th>also</th>\n",
       "      <th>also noting</th>\n",
       "      <th>americans</th>\n",
       "      <th>americans also</th>\n",
       "      <th>...</th>\n",
       "      <th>witnesses want</th>\n",
       "      <th>work</th>\n",
       "      <th>work investigating</th>\n",
       "      <th>worried</th>\n",
       "      <th>worried populace</th>\n",
       "      <th>year</th>\n",
       "      <th>year country</th>\n",
       "      <th>year old</th>\n",
       "      <th>yet</th>\n",
       "      <th>yet another</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 382 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   adding  adding wants  administration  administration official  advisor  \\\n",
       "0       0             0               1                        1        1   \n",
       "1       1             1               0                        0        0   \n",
       "\n",
       "   advisor michael  also  also noting  americans  americans also  ...  \\\n",
       "0                1     1            1          1               1  ...   \n",
       "1                0     0            0          0               0  ...   \n",
       "\n",
       "   witnesses want  work  work investigating  worried  worried populace  year  \\\n",
       "0               0     0                   0        1                 1     2   \n",
       "1               1     1                   1        0                 0     0   \n",
       "\n",
       "   year country  year old  yet  yet another  \n",
       "0             1         1    0            0  \n",
       "1             0         0    1            1  \n",
       "\n",
       "[2 rows x 382 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec = CountVectorizer(token_pattern=r\"([a-zA-Z]+(?:'[a-z]+)?)\", stop_words=sw,\n",
    "                      ngram_range=[1, 2])\n",
    "X = vec.fit_transform(corpus.body[0:2])\n",
    "\n",
    "df = pd.DataFrame(X.toarray(), columns=vec.get_feature_names())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our document term matrix gets bigger and bigger, with more and more zeros, becoming sparser and sparser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aa</th>\n",
       "      <th>aaaaaaah</th>\n",
       "      <th>aaaaaah</th>\n",
       "      <th>aaaaargh</th>\n",
       "      <th>aaaah</th>\n",
       "      <th>aaah</th>\n",
       "      <th>aaargh</th>\n",
       "      <th>aah</th>\n",
       "      <th>aahing</th>\n",
       "      <th>aap</th>\n",
       "      <th>...</th>\n",
       "      <th>zoos</th>\n",
       "      <th>zor</th>\n",
       "      <th>zozovitch</th>\n",
       "      <th>zte</th>\n",
       "      <th>zuckerberg</th>\n",
       "      <th>zuercher</th>\n",
       "      <th>zverev</th>\n",
       "      <th>zych</th>\n",
       "      <th>zzouss</th>\n",
       "      <th>zzzzzst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23458 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   aa  aaaaaaah  aaaaaah  aaaaargh  aaaah  aaah  aaargh  aah  aahing  aap  \\\n",
       "0   0         0        0         0      0     0       0    0       0    0   \n",
       "1   0         0        0         0      0     0       0    0       0    0   \n",
       "2   0         0        0         0      0     0       0    0       0    0   \n",
       "3   0         0        0         0      0     0       0    0       0    0   \n",
       "4   0         0        0         0      0     0       0    0       0    0   \n",
       "\n",
       "   ...  zoos  zor  zozovitch  zte  zuckerberg  zuercher  zverev  zych  zzouss  \\\n",
       "0  ...     0    0          0    0           0         0       0     0       0   \n",
       "1  ...     0    0          0    0           0         0       0     0       0   \n",
       "2  ...     0    0          0    0           0         0       0     0       0   \n",
       "3  ...     0    0          0    0           0         0       0     0       0   \n",
       "4  ...     0    0          0    0           0         0       0     0       0   \n",
       "\n",
       "   zzzzzst  \n",
       "0        0  \n",
       "1        0  \n",
       "2        0  \n",
       "3        0  \n",
       "4        0  \n",
       "\n",
       "[5 rows x 23458 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec = CountVectorizer(token_pattern=r\"([a-zA-Z]+(?:'[a-z]+)?)\", stop_words=sw,\n",
    "                      ngram_range=[1, 1])\n",
    "# Now fit to the entire corpus\n",
    "X = vec.fit_transform(corpus.body)\n",
    "\n",
    "df = pd.DataFrame(X.toarray(), columns=vec.get_feature_names())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can set upper and lower limits to the word frequency:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      Noting that the resignation of James Mattis as...\n",
       "1      Desperate to unwind after months of nonstop wo...\n",
       "2      Nearly halfway through his presidential term, ...\n",
       "3      Attempting to make amends for gross abuses of ...\n",
       "4      Decrying the Senate’s resolution blaming the c...\n",
       "                             ...                        \n",
       "995     Britain’s opposition leader Jeremy Corbyn wou...\n",
       "996     Turkey will take over the fight against Islam...\n",
       "997     Malaysia is seeking $7.5 billion in reparatio...\n",
       "998     An Israeli court sentenced a Palestinian to 1...\n",
       "999     At least 22 people have died due to landslide...\n",
       "Name: body, Length: 1000, dtype: object"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aapl</th>\n",
       "      <th>aaron</th>\n",
       "      <th>aaron ross</th>\n",
       "      <th>ab</th>\n",
       "      <th>abandon</th>\n",
       "      <th>abandon conservatives</th>\n",
       "      <th>abandoned</th>\n",
       "      <th>abandoned grassroots</th>\n",
       "      <th>abandoning</th>\n",
       "      <th>abandoning quarter</th>\n",
       "      <th>...</th>\n",
       "      <th>zone</th>\n",
       "      <th>zone eu</th>\n",
       "      <th>zones</th>\n",
       "      <th>zoo</th>\n",
       "      <th>zoo closed</th>\n",
       "      <th>zooming</th>\n",
       "      <th>zor</th>\n",
       "      <th>zte</th>\n",
       "      <th>zte corp</th>\n",
       "      <th>zuckerberg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 31392 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     aapl  aaron  aaron ross  ab  abandon  abandon conservatives  abandoned  \\\n",
       "0       0      0           0   0        0                      0          0   \n",
       "1       0      0           0   0        0                      0          0   \n",
       "2       0      0           0   0        0                      0          0   \n",
       "3       0      0           0   0        0                      0          0   \n",
       "4       0      0           0   0        0                      0          0   \n",
       "..    ...    ...         ...  ..      ...                    ...        ...   \n",
       "995     0      0           0   0        0                      0          0   \n",
       "996     0      0           0   0        0                      0          0   \n",
       "997     0      0           0   0        0                      0          0   \n",
       "998     0      0           0   0        0                      0          0   \n",
       "999     0      0           0   0        0                      0          0   \n",
       "\n",
       "     abandoned grassroots  abandoning  abandoning quarter  ...  zone  zone eu  \\\n",
       "0                       0           0                   0  ...     0        0   \n",
       "1                       0           0                   0  ...     0        0   \n",
       "2                       0           0                   0  ...     0        0   \n",
       "3                       0           0                   0  ...     0        0   \n",
       "4                       0           0                   0  ...     0        0   \n",
       "..                    ...         ...                 ...  ...   ...      ...   \n",
       "995                     0           0                   0  ...     0        0   \n",
       "996                     0           0                   0  ...     0        0   \n",
       "997                     0           0                   0  ...     0        0   \n",
       "998                     0           0                   0  ...     0        0   \n",
       "999                     0           0                   0  ...     0        0   \n",
       "\n",
       "     zones  zoo  zoo closed  zooming  zor  zte  zte corp  zuckerberg  \n",
       "0        0    0           0        0    0    0         0           0  \n",
       "1        0    0           0        0    0    0         0           0  \n",
       "2        0    0           0        0    0    0         0           0  \n",
       "3        0    0           0        0    0    0         0           0  \n",
       "4        0    0           0        0    0    0         0           0  \n",
       "..     ...  ...         ...      ...  ...  ...       ...         ...  \n",
       "995      0    0           0        0    0    0         0           0  \n",
       "996      0    0           0        0    0    0         0           0  \n",
       "997      0    0           0        0    0    0         0           0  \n",
       "998      0    0           0        0    0    0         0           0  \n",
       "999      0    0           0        0    0    0         0           0  \n",
       "\n",
       "[1000 rows x 31392 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec = CountVectorizer(token_pattern=r\"([a-zA-Z]+(?:'[a-z]+)?)\",\n",
    "                      stop_words=sw, ngram_range=[1, 2],\n",
    "                      min_df=2, max_df=25)\n",
    "X = vec.fit_transform(corpus.body)\n",
    "\n",
    "df_cv = pd.DataFrame(X.toarray(), columns=vec.get_feature_names())\n",
    "df_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `TfidfVectorizer`\n",
    "\n",
    "There are many schemas for determining the values of each entry in a document term matrix, and one of the most common uses the TF-IDF algorithm -- \"Term Frequency-Inverse Document Frequency\". Essentially, tf-idf *normalizes* the raw count of the document term matrix. And it represents how important a word is in the given document. \n",
    "\n",
    "> The goal of using tf-idf instead of the raw frequencies of occurrence of a token in a given document is to scale down the impact of tokens that occur very frequently in a given corpus and that are hence empirically less informative than features that occur in a small fraction of the training corpus.\n",
    "\n",
    "- TF (Term Frequency)\n",
    "Term frequency is the frequency of the word in the document divided by the total words in the document.\n",
    "\n",
    "- IDF (inverse document frequency)\n",
    "Inverse document frequency is a measure of how much information the word provides, i.e., if it's common or rare across all documents. It is generally calculated as the logarithmically scaled inverse fraction of the documents that contain the word (obtained by dividing the total number of documents by the number of documents containing the term, and then taking the logarithm of that quotient):\n",
    "\n",
    "$$idf(w) = log (\\frac{number\\ of\\ documents}{num\\ of\\ documents\\ containing\\ w})$$\n",
    "\n",
    "tf-idf is the product of term frequency and inverse document frequency, or tf * idf. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aa</th>\n",
       "      <th>aaaaaaah</th>\n",
       "      <th>aaaaaah</th>\n",
       "      <th>aaaaargh</th>\n",
       "      <th>aaaah</th>\n",
       "      <th>aaah</th>\n",
       "      <th>aaargh</th>\n",
       "      <th>aah</th>\n",
       "      <th>aahing</th>\n",
       "      <th>aap</th>\n",
       "      <th>...</th>\n",
       "      <th>zoos</th>\n",
       "      <th>zor</th>\n",
       "      <th>zozovitch</th>\n",
       "      <th>zte</th>\n",
       "      <th>zuckerberg</th>\n",
       "      <th>zuercher</th>\n",
       "      <th>zverev</th>\n",
       "      <th>zych</th>\n",
       "      <th>zzouss</th>\n",
       "      <th>zzzzzst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23458 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    aa  aaaaaaah  aaaaaah  aaaaargh  aaaah  aaah  aaargh  aah  aahing  aap  \\\n",
       "0  0.0       0.0      0.0       0.0    0.0   0.0     0.0  0.0     0.0  0.0   \n",
       "1  0.0       0.0      0.0       0.0    0.0   0.0     0.0  0.0     0.0  0.0   \n",
       "2  0.0       0.0      0.0       0.0    0.0   0.0     0.0  0.0     0.0  0.0   \n",
       "3  0.0       0.0      0.0       0.0    0.0   0.0     0.0  0.0     0.0  0.0   \n",
       "4  0.0       0.0      0.0       0.0    0.0   0.0     0.0  0.0     0.0  0.0   \n",
       "\n",
       "   ...  zoos  zor  zozovitch  zte  zuckerberg  zuercher  zverev  zych  zzouss  \\\n",
       "0  ...   0.0  0.0        0.0  0.0         0.0       0.0     0.0   0.0     0.0   \n",
       "1  ...   0.0  0.0        0.0  0.0         0.0       0.0     0.0   0.0     0.0   \n",
       "2  ...   0.0  0.0        0.0  0.0         0.0       0.0     0.0   0.0     0.0   \n",
       "3  ...   0.0  0.0        0.0  0.0         0.0       0.0     0.0   0.0     0.0   \n",
       "4  ...   0.0  0.0        0.0  0.0         0.0       0.0     0.0   0.0     0.0   \n",
       "\n",
       "   zzzzzst  \n",
       "0      0.0  \n",
       "1      0.0  \n",
       "2      0.0  \n",
       "3      0.0  \n",
       "4      0.0  \n",
       "\n",
       "[5 rows x 23458 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_vec = TfidfVectorizer(token_pattern=r\"([a-zA-Z]+(?:'[a-z]+)?)\", stop_words=sw)\n",
    "X = tf_vec.fit_transform(corpus.body)\n",
    "\n",
    "df = pd.DataFrame(X.toarray(), columns=tf_vec.get_feature_names())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Power nerds, these are a special breed of techie nerds who want to take over the world with their technological prowess. From the birth of the World Wide Web, these power nerds have created and grown companies that are now monopolies controlling every facet of people’s lives. Smartphones, apps, search engines, social networks – power nerds are in everything, their power increases daily as more millions of people use their networks. Power nerds are ruthless, they are creatures who do not balk in crushing their opponents completely without mercy and their greed for complete controlling power over everything is boundless. To quote a few examples of companies that are run by ruthless power nerds, we can of course cite Facebook, Twitter, Amazon and Google. These companies are not only seeking to rule and control everything, they also are using their power to manipulate data taken from their platforms to make money and increase their influence, as well as shut down any voices that are not left leaning. All Hail Zuckerberg “Power nerds are inherently evil. Zuckerberg is one example of a power nerd so power hungry that he pursues global domination with a vehement ruthless nasty streak. These tech robots are machines, they are not really human anymore, their fuel is pure power and more power, and they will use billions of people to achieve their goals at all cost. Tech power nerds are farmers of people, they farm billions of people for data,” an observer of the current situation revealed. Power nerds also do not have a problem about farming data from billions of people without their knowledge, they also abuse their positions to be politically biased and censor free speech as a means of gaining even more power. One can only hope that companies like Facebook one day are brought to justice for their evil, devious crimes committed against billions of people. In 2009, Facebook was caught lying to their account holders about the amount and type of information it was collecting on them, and the company also explicitly lied about who they were providing that information to. As a result, the Federal Trade Commission censured the company in 2011 for violations of Article 5 of the Federal Trade Commission Act. The core mission of Article 5 of the FTC Act is to protect consumer welfare and prevent unfair business acts or practices from occurring. In defiance of the Federal Trade Commission’s order, Facebook continued to reveal their customers private account information to unauthorized individuals and corporations and is therefore liable for civil penalties of $41,484 per each violation – a fine that could reach $3 trillion Dollars. Talking about fines, Google, a company that controls 91.5% of search traffic in Europe alone, is being slapped with antitrust fines from the EU, but it’s only for a measly £2.14 billion, which for a company that pays literally no tax, is peanuts. At the end of the day, these companies led by power nerds have now spread their octopus-like grip over the whole globe, and to even begin deconstructing their evil plan of complete control, will be nearly impossible now unless these companies are fined, broken up and told to pay the tax they owe. Hopefully one day the power nerds are put in their place, and we can all breathe a breath of fresh air on a free internet once again.'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.iloc[313].body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nerds         0.600312\n",
       "power         0.396376\n",
       "companies     0.168090\n",
       "billions      0.140697\n",
       "facebook      0.129217\n",
       "ruthless      0.127204\n",
       "company       0.106695\n",
       "evil          0.104667\n",
       "zuckerberg    0.098663\n",
       "people        0.095626\n",
       "Name: 313, dtype: float64"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[313].sort_values(ascending=False)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare the tfidf to the count vectorizer output for one document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aa</th>\n",
       "      <th>aaaaaaah</th>\n",
       "      <th>aaaaaah</th>\n",
       "      <th>aaaaargh</th>\n",
       "      <th>aaaah</th>\n",
       "      <th>aaah</th>\n",
       "      <th>aaargh</th>\n",
       "      <th>aah</th>\n",
       "      <th>aahing</th>\n",
       "      <th>aap</th>\n",
       "      <th>...</th>\n",
       "      <th>zoos</th>\n",
       "      <th>zor</th>\n",
       "      <th>zozovitch</th>\n",
       "      <th>zte</th>\n",
       "      <th>zuckerberg</th>\n",
       "      <th>zuercher</th>\n",
       "      <th>zverev</th>\n",
       "      <th>zych</th>\n",
       "      <th>zzouss</th>\n",
       "      <th>zzzzzst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 23458 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     aa  aaaaaaah  aaaaaah  aaaaargh  aaaah  aaah  aaargh  aah  aahing  aap  \\\n",
       "0     0         0        0         0      0     0       0    0       0    0   \n",
       "1     0         0        0         0      0     0       0    0       0    0   \n",
       "2     0         0        0         0      0     0       0    0       0    0   \n",
       "3     0         0        0         0      0     0       0    0       0    0   \n",
       "4     0         0        0         0      0     0       0    0       0    0   \n",
       "..   ..       ...      ...       ...    ...   ...     ...  ...     ...  ...   \n",
       "995   0         0        0         0      0     0       0    0       0    0   \n",
       "996   0         0        0         0      0     0       0    0       0    0   \n",
       "997   0         0        0         0      0     0       0    0       0    0   \n",
       "998   0         0        0         0      0     0       0    0       0    0   \n",
       "999   0         0        0         0      0     0       0    0       0    0   \n",
       "\n",
       "     ...  zoos  zor  zozovitch  zte  zuckerberg  zuercher  zverev  zych  \\\n",
       "0    ...     0    0          0    0           0         0       0     0   \n",
       "1    ...     0    0          0    0           0         0       0     0   \n",
       "2    ...     0    0          0    0           0         0       0     0   \n",
       "3    ...     0    0          0    0           0         0       0     0   \n",
       "4    ...     0    0          0    0           0         0       0     0   \n",
       "..   ...   ...  ...        ...  ...         ...       ...     ...   ...   \n",
       "995  ...     0    0          0    0           0         0       0     0   \n",
       "996  ...     0    0          0    0           0         0       0     0   \n",
       "997  ...     0    0          0    0           0         0       0     0   \n",
       "998  ...     0    0          0    0           0         0       0     0   \n",
       "999  ...     0    0          0    0           0         0       0     0   \n",
       "\n",
       "     zzouss  zzzzzst  \n",
       "0         0        0  \n",
       "1         0        0  \n",
       "2         0        0  \n",
       "3         0        0  \n",
       "4         0        0  \n",
       "..      ...      ...  \n",
       "995       0        0  \n",
       "996       0        0  \n",
       "997       0        0  \n",
       "998       0        0  \n",
       "999       0        0  \n",
       "\n",
       "[1000 rows x 23458 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec = CountVectorizer(token_pattern=r\"([a-zA-Z]+(?:'[a-z]+)?)\", stop_words=sw)\n",
    "X = vec.fit_transform(corpus.body)\n",
    "\n",
    "df_cv = pd.DataFrame(X.toarray(), columns=vec.get_feature_names())\n",
    "df_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "power        18\n",
       "nerds        11\n",
       "people        7\n",
       "companies     6\n",
       "one           4\n",
       "also          4\n",
       "billions      4\n",
       "facebook      4\n",
       "company       4\n",
       "data          3\n",
       "Name: 313, dtype: int64"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cv.iloc[313].sort_values(ascending=False)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tfidf lessoned the importance of some of the more common words, including a word, \"also\", which might have made it into the stopword list.\n",
    "\n",
    "It also assigns \"nerds\" more weight than power.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Nerds\" only shows up in document 313: 1 document.\n",
      "\"Power\" shows up in 147 documents!\n"
     ]
    }
   ],
   "source": [
    "print(f'\"Nerds\" only shows up in document 313: {len(df_cv[df.nerds!=0])} document.')\n",
    "print(f'\"Power\" shows up in {len(df_cv[df.power!=0])} documents!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the words are stored in a `.vocabulary_` attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'noting': 14228,\n",
       " 'resignation': 17453,\n",
       " 'james': 11083,\n",
       " 'mattis': 12876,\n",
       " 'secretary': 18493,\n",
       " 'defense': 5328,\n",
       " 'marked': 12753,\n",
       " 'ouster': 14711,\n",
       " 'third': 21062,\n",
       " 'top': 21291,\n",
       " 'administration': 293,\n",
       " 'official': 14445,\n",
       " 'less': 12042,\n",
       " 'three': 21096,\n",
       " 'weeks': 22859,\n",
       " 'worried': 23186,\n",
       " 'populace': 15784,\n",
       " 'told': 21257,\n",
       " 'reporters': 17365,\n",
       " 'friday': 8389,\n",
       " 'unsure': 22126,\n",
       " 'many': 12705,\n",
       " 'former': 8261,\n",
       " 'trump': 21633,\n",
       " 'staffers': 19832,\n",
       " 'could': 4654,\n",
       " 'safely': 18072,\n",
       " 'reabsorb': 16838,\n",
       " 'jesus': 11159,\n",
       " 'take': 20693,\n",
       " 'back': 1540,\n",
       " 'assholes': 1254,\n",
       " 'need': 13936,\n",
       " 'time': 21177,\n",
       " 'process': 16146,\n",
       " 'one': 14516,\n",
       " 'get': 8762,\n",
       " 'next': 14045,\n",
       " 'said': 18088,\n",
       " 'year': 23301,\n",
       " 'old': 14477,\n",
       " 'gregory': 9106,\n",
       " 'birch': 2136,\n",
       " 'naperville': 13830,\n",
       " 'il': 10220,\n",
       " 'echoing': 6555,\n",
       " 'concerns': 4203,\n",
       " 'million': 13240,\n",
       " 'americans': 723,\n",
       " 'also': 658,\n",
       " 'country': 4674,\n",
       " 'truly': 21632,\n",
       " 'beginning': 1905,\n",
       " 'reintegrate': 17182,\n",
       " 'national': 13870,\n",
       " 'security': 18511,\n",
       " 'advisor': 365,\n",
       " 'michael': 13158,\n",
       " 'flynn': 8141,\n",
       " 'sustainable': 20528,\n",
       " 'say': 18259,\n",
       " 'handle': 9392,\n",
       " 'maybe': 12900,\n",
       " 'two': 21763,\n",
       " 'members': 13034,\n",
       " 'inner': 10655,\n",
       " 'circle': 3622,\n",
       " 'remainder': 17248,\n",
       " 'limits': 12176,\n",
       " 'u': 21783,\n",
       " 'confirmed': 4262,\n",
       " 'pieces': 15466,\n",
       " 'shit': 18878,\n",
       " 'trying': 21650,\n",
       " 'rejoin': 17197,\n",
       " 'society': 19403,\n",
       " 'desperate': 5613,\n",
       " 'unwind': 22154,\n",
       " 'months': 13509,\n",
       " 'nonstop': 14171,\n",
       " 'work': 23165,\n",
       " 'investigating': 10907,\n",
       " 'russian': 18013,\n",
       " 'influence': 10582,\n",
       " 'election': 6661,\n",
       " 'visibly': 22540,\n",
       " 'exhausted': 7345,\n",
       " 'special': 19603,\n",
       " 'counsel': 4657,\n",
       " 'robert': 17792,\n",
       " 'mueller': 13661,\n",
       " 'powered': 15893,\n",
       " 'phone': 15415,\n",
       " 'order': 14631,\n",
       " 'give': 8824,\n",
       " 'break': 2565,\n",
       " 'news': 14036,\n",
       " 'concerning': 4202,\n",
       " 'probe': 16133,\n",
       " 'holiday': 9845,\n",
       " 'last': 11836,\n",
       " 'thing': 21050,\n",
       " 'want': 22725,\n",
       " 'spending': 19652,\n",
       " 'family': 7625,\n",
       " 'cascade': 3151,\n",
       " 'push': 16491,\n",
       " 'notifications': 14225,\n",
       " 'telling': 20887,\n",
       " 'yet': 23328,\n",
       " 'another': 885,\n",
       " 'oligarch': 14488,\n",
       " 'political': 15735,\n",
       " 'operative': 14567,\n",
       " 'highly': 9751,\n",
       " 'placed': 15559,\n",
       " 'socialite': 19399,\n",
       " 'used': 22234,\n",
       " 'deutsche': 5689,\n",
       " 'bank': 1676,\n",
       " 'channels': 3382,\n",
       " 'funnel': 8496,\n",
       " 'money': 13473,\n",
       " 'campaign': 2971,\n",
       " 'fbi': 7737,\n",
       " 'director': 5871,\n",
       " 'firmly': 7979,\n",
       " 'holding': 9839,\n",
       " 'power': 15892,\n",
       " 'button': 2865,\n",
       " 'adding': 256,\n",
       " 'wants': 22728,\n",
       " 'completely': 4131,\n",
       " 'present': 16004,\n",
       " 'moment': 13455,\n",
       " 'celebrating': 3264,\n",
       " 'loved': 12395,\n",
       " 'ones': 14518,\n",
       " 'ruminating': 17978,\n",
       " 'met': 13111,\n",
       " 'diplomat': 5856,\n",
       " 'whether': 22938,\n",
       " 'someone': 19465,\n",
       " 'using': 22245,\n",
       " 'social': 19393,\n",
       " 'media': 12975,\n",
       " 'tamper': 20730,\n",
       " 'witnesses': 23117,\n",
       " 'calm': 2949,\n",
       " 'even': 7226,\n",
       " 'think': 21054,\n",
       " 'individual': 10507,\n",
       " 'name': 13816,\n",
       " 'wait': 22678,\n",
       " 'hear': 9574,\n",
       " 'important': 10341,\n",
       " 'developments': 5704,\n",
       " 'january': 11102,\n",
       " 'since': 19086,\n",
       " 'know': 11618,\n",
       " 'second': 18484,\n",
       " 'read': 16854,\n",
       " 'something': 19468,\n",
       " 'eric': 7092,\n",
       " 'involved': 10933,\n",
       " 'deeply': 5297,\n",
       " 'previously': 16054,\n",
       " 'suspected': 20514,\n",
       " 'pulled': 16404,\n",
       " 'ruin': 17961,\n",
       " 'whole': 22978,\n",
       " 'vacation': 22277,\n",
       " 'press': 16020,\n",
       " 'reactivated': 16850,\n",
       " 'check': 3449,\n",
       " 'real': 16871,\n",
       " 'quick': 16605,\n",
       " 'nearly': 13921,\n",
       " 'halfway': 9341,\n",
       " 'presidential': 16018,\n",
       " 'term': 20941,\n",
       " 'donald': 6217,\n",
       " 'continued': 4456,\n",
       " 'exist': 7356,\n",
       " 'perpetual': 15309,\n",
       " 'state': 19914,\n",
       " 'controversy': 4495,\n",
       " 'provided': 16336,\n",
       " 'shortage': 18915,\n",
       " 'outrageous': 14751,\n",
       " 'moments': 13458,\n",
       " 'onion': 14523,\n",
       " 'looks': 12345,\n",
       " 'significant': 19040,\n",
       " 'events': 7231,\n",
       " 'presidency': 16016,\n",
       " 'attempting': 1341,\n",
       " 'make': 12593,\n",
       " 'amends': 719,\n",
       " 'gross': 9152,\n",
       " 'abuses': 101,\n",
       " 'interior': 10820,\n",
       " 'department': 5521,\n",
       " 'unusually': 22144,\n",
       " 'contrite': 4486,\n",
       " 'ryan': 18024,\n",
       " 'zinke': 23434,\n",
       " 'apologized': 957,\n",
       " 'monday': 13468,\n",
       " 'misusing': 13358,\n",
       " 'government': 8992,\n",
       " 'funds': 8491,\n",
       " 'sending': 18590,\n",
       " 'ethics': 7173,\n",
       " 'committee': 4045,\n",
       " 'vase': 22336,\n",
       " 'change': 3377,\n",
       " 'anything': 931,\n",
       " 'exploited': 7424,\n",
       " 'cabinet': 2896,\n",
       " 'position': 15832,\n",
       " 'hope': 9923,\n",
       " 'accept': 121,\n",
       " 'beautiful': 1863,\n",
       " 'example': 7280,\n",
       " 'qing': 16543,\n",
       " 'dynasty': 6492,\n",
       " 'porcelain': 15798,\n",
       " 'small': 19289,\n",
       " 'token': 21254,\n",
       " 'regret': 17136,\n",
       " 'acknowledging': 198,\n",
       " 'gift': 8793,\n",
       " 'spent': 19654,\n",
       " 'taxpayer': 20807,\n",
       " 'renovate': 17308,\n",
       " 'office': 14441,\n",
       " 'doors': 6245,\n",
       " 'hoped': 9924,\n",
       " 'would': 23204,\n",
       " 'consider': 4344,\n",
       " 'sincere': 19087,\n",
       " 'gesture': 8758,\n",
       " 'apology': 958,\n",
       " 'wrong': 23239,\n",
       " 'advantage': 337,\n",
       " 'lustrous': 12464,\n",
       " 'glazing': 8844,\n",
       " 'firing': 7975,\n",
       " 'evident': 7250,\n",
       " 'piece': 15464,\n",
       " 'move': 13612,\n",
       " 'forgive': 8243,\n",
       " 'human': 10048,\n",
       " 'failings': 7581,\n",
       " 'please': 15621,\n",
       " 'remember': 17263,\n",
       " 'man': 12632,\n",
       " 'detail': 5643,\n",
       " 'turkey': 21704,\n",
       " 'violated': 22508,\n",
       " 'hatch': 9504,\n",
       " 'act': 218,\n",
       " 'acted': 219,\n",
       " 'pawn': 15150,\n",
       " 'oil': 14466,\n",
       " 'gas': 8620,\n",
       " 'industry': 10535,\n",
       " 'rather': 16801,\n",
       " 'eyes': 7521,\n",
       " 'happen': 9420,\n",
       " 'fall': 7606,\n",
       " 'unique': 22012,\n",
       " 'kaolin': 11395,\n",
       " 'clay': 3717,\n",
       " 'bought': 2470,\n",
       " 'mercedes': 13071,\n",
       " 'benz': 2000,\n",
       " 'sedans': 18513,\n",
       " 'find': 7939,\n",
       " 'parking': 15027,\n",
       " 'lot': 12376,\n",
       " 'leave': 11943,\n",
       " 'today': 21243,\n",
       " 'plans': 15585,\n",
       " 'apologize': 956,\n",
       " 'person': 15324,\n",
       " 'member': 13033,\n",
       " 'visiting': 22545,\n",
       " 'homes': 9871,\n",
       " 'helicopter': 9642,\n",
       " 'decrying': 5282,\n",
       " 'senate': 18586,\n",
       " 'resolution': 17465,\n",
       " 'blaming': 2196,\n",
       " 'crown': 4882,\n",
       " 'prince': 16086,\n",
       " 'brutal': 2707,\n",
       " 'torture': 21323,\n",
       " 'murder': 13714,\n",
       " 'journalist': 11260,\n",
       " 'jamal': 11082,\n",
       " 'khashoggi': 11497,\n",
       " 'cruel': 4892,\n",
       " 'inhumane': 10628,\n",
       " 'unprecedented': 22066,\n",
       " 'interference': 10816,\n",
       " 'sovereign': 19541,\n",
       " 'kingdom': 11552,\n",
       " 'internal': 10829,\n",
       " 'affairs': 385,\n",
       " 'launched': 11861,\n",
       " 'rights': 17710,\n",
       " 'investigation': 10908,\n",
       " 'harsh': 9479,\n",
       " 'treatment': 21525,\n",
       " 'saudi': 18234,\n",
       " 'ruler': 17970,\n",
       " 'mohammad': 13437,\n",
       " 'bin': 2114,\n",
       " 'salman': 18122,\n",
       " 'looking': 12343,\n",
       " 'troubling': 21615,\n",
       " 'accusations': 175,\n",
       " 'united': 22019,\n",
       " 'states': 19919,\n",
       " 'chosen': 3574,\n",
       " 'willfully': 23027,\n",
       " 'knowingly': 11621,\n",
       " 'place': 15558,\n",
       " 'fault': 7717,\n",
       " 'dissident': 6076,\n",
       " 'president': 16017,\n",
       " 'claiming': 3666,\n",
       " 'despot': 5621,\n",
       " 'made': 12517,\n",
       " 'endure': 6887,\n",
       " 'loss': 12373,\n",
       " 'military': 13222,\n",
       " 'funding': 8489,\n",
       " 'ongoing': 14522,\n",
       " 'war': 22730,\n",
       " 'yemen': 23318,\n",
       " 'left': 11961,\n",
       " 'millions': 13242,\n",
       " 'homeless': 9869,\n",
       " 'starving': 19911,\n",
       " 'matter': 12871,\n",
       " 'whose': 22988,\n",
       " 'dismemberment': 6010,\n",
       " 'may': 12898,\n",
       " 'ordered': 14632,\n",
       " 'facing': 7551,\n",
       " 'criticism': 4846,\n",
       " 'like': 12153,\n",
       " 'international': 10831,\n",
       " 'stage': 19836,\n",
       " 'powerful': 15894,\n",
       " 'leader': 11911,\n",
       " 'basically': 1775,\n",
       " 'kind': 11543,\n",
       " 'mistreatment': 13352,\n",
       " 'seriously': 18662,\n",
       " 'treat': 21521,\n",
       " 'authoritarian': 1414,\n",
       " 'regimes': 17122,\n",
       " 'purchase': 16456,\n",
       " 'weapons': 22825,\n",
       " 'without': 23113,\n",
       " 'billions': 2109,\n",
       " 'dollars': 6201,\n",
       " 'aid': 476,\n",
       " 'regime': 17121,\n",
       " 'supposed': 20451,\n",
       " 'maintain': 12575,\n",
       " 'basic': 1774,\n",
       " 'standard': 19873,\n",
       " 'living': 12262,\n",
       " 'expected': 7379,\n",
       " 'charge': 3406,\n",
       " 'american': 722,\n",
       " 'senators': 18588,\n",
       " 'crimes': 4817,\n",
       " 'humanity': 10052,\n",
       " 'role': 17833,\n",
       " 'responsible': 17500,\n",
       " 'actions': 222,\n",
       " 'following': 8174,\n",
       " 'sentencing': 18619,\n",
       " 'hush': 10105,\n",
       " 'scandal': 18274,\n",
       " 'cohen': 3891,\n",
       " 'granted': 9046,\n",
       " 'prison': 16108,\n",
       " 'release': 17215,\n",
       " 'new': 14026,\n",
       " 'job': 11202,\n",
       " 'sources': 19524,\n",
       " 'wednesday': 22851,\n",
       " 'confident': 4254,\n",
       " 'engaging': 6904,\n",
       " 'honest': 9890,\n",
       " 'help': 9654,\n",
       " 'mr': 13632,\n",
       " 'rehabilitation': 17155,\n",
       " 'warden': 22733,\n",
       " 'pete': 15361,\n",
       " 'clements': 3743,\n",
       " 'opportunity': 14586,\n",
       " 'serving': 18675,\n",
       " 'see': 18519,\n",
       " 'error': 7110,\n",
       " 'past': 15089,\n",
       " 'behaviors': 1915,\n",
       " 'arrives': 1148,\n",
       " 'march': 12723,\n",
       " 'bused': 2836,\n",
       " 'penitentiary': 15238,\n",
       " 'manhattan': 12665,\n",
       " 'eight': 6632,\n",
       " 'hour': 9991,\n",
       " 'day': 5170,\n",
       " 'returning': 17576,\n",
       " 'night': 14097,\n",
       " 'strict': 20158,\n",
       " 'supervision': 20432,\n",
       " 'furloughs': 8508,\n",
       " 'allow': 623,\n",
       " 'use': 22233,\n",
       " 'skills': 19166,\n",
       " 'betterment': 2048,\n",
       " 'community': 4070,\n",
       " 'chance': 3372,\n",
       " 'added': 251,\n",
       " 'request': 17401,\n",
       " 'rnc': 17772,\n",
       " 'deputy': 5565,\n",
       " 'finance': 7931,\n",
       " 'chairman': 3344,\n",
       " 'denied': 5483,\n",
       " 'environment': 7028,\n",
       " 'easy': 6532,\n",
       " 'backslide': 1565,\n",
       " 'criminality': 4821,\n",
       " 'grimacing': 9130,\n",
       " 'clutching': 3828,\n",
       " 'shoulder': 18931,\n",
       " 'fox': 8308,\n",
       " 'nfl': 14047,\n",
       " 'announcer': 869,\n",
       " 'joe': 11213,\n",
       " 'buck': 2725,\n",
       " 'tore': 21306,\n",
       " 'rotator': 17896,\n",
       " 'cuff': 4941,\n",
       " 'awkward': 1495,\n",
       " 'throw': 21113,\n",
       " 'sideline': 19003,\n",
       " 'quarter': 16573,\n",
       " 'buccaneers': 2722,\n",
       " 'vs': 22643,\n",
       " 'cowboys': 4723,\n",
       " 'game': 8585,\n",
       " 'hate': 9506,\n",
       " 'go': 8898,\n",
       " 'especially': 7134,\n",
       " 'routine': 17916,\n",
       " 'erin': 7095,\n",
       " 'field': 7873,\n",
       " 'conditions': 4236,\n",
       " 'thousand': 21088,\n",
       " 'times': 21185,\n",
       " 'commentator': 4022,\n",
       " 'troy': 21622,\n",
       " 'aikman': 484,\n",
       " 'went': 22896,\n",
       " 'hard': 9439,\n",
       " 'stumbling': 20219,\n",
       " 'first': 7981,\n",
       " 'words': 23162,\n",
       " 'sentence': 18616,\n",
       " 'still': 20008,\n",
       " 'ground': 9158,\n",
       " 'writhing': 23236,\n",
       " 'pain': 14896,\n",
       " 'update': 22160,\n",
       " 'mouth': 13607,\n",
       " 'look': 12341,\n",
       " 'right': 17707,\n",
       " 'twisted': 21758,\n",
       " 'awkwardly': 1496,\n",
       " 'shock': 18888,\n",
       " 'crossed': 4868,\n",
       " 'face': 7536,\n",
       " 'bad': 1578,\n",
       " 'saw': 18253,\n",
       " 'al': 521,\n",
       " 'michaels': 13159,\n",
       " 'tear': 20826,\n",
       " 'acl': 199,\n",
       " 'touchdown': 21342,\n",
       " 'call': 2942,\n",
       " 'way': 22806,\n",
       " 'going': 8926,\n",
       " 'announcing': 872,\n",
       " 'least': 11942,\n",
       " 'month': 13507,\n",
       " 'treated': 21522,\n",
       " 'concussion': 4225,\n",
       " 'analyze': 790,\n",
       " 'play': 15603,\n",
       " 'conversion': 4515,\n",
       " 'categorically': 3204,\n",
       " 'denying': 5517,\n",
       " 'allegations': 599,\n",
       " 'tactic': 20667,\n",
       " 'unconstitutional': 21862,\n",
       " 'unfairly': 21955,\n",
       " 'targeted': 20762,\n",
       " 'players': 15608,\n",
       " 'protested': 16317,\n",
       " 'anthem': 900,\n",
       " 'commissioner': 4036,\n",
       " 'roger': 17824,\n",
       " 'goodell': 8950,\n",
       " 'released': 17216,\n",
       " 'statement': 19917,\n",
       " 'sunday': 20390,\n",
       " 'defending': 5326,\n",
       " 'subject': 20245,\n",
       " 'panthers': 14959,\n",
       " 'safety': 18076,\n",
       " 'reid': 17160,\n",
       " 'random': 16743,\n",
       " 'stop': 20061,\n",
       " 'frisk': 8409,\n",
       " 'searches': 18460,\n",
       " 'simply': 19079,\n",
       " 'keep': 11442,\n",
       " 'clean': 3720,\n",
       " 'provide': 16335,\n",
       " 'safe': 18067,\n",
       " 'benefits': 1985,\n",
       " 'case': 3153,\n",
       " 'received': 16936,\n",
       " 'anonymous': 882,\n",
       " 'tip': 21206,\n",
       " 'suspicious': 20524,\n",
       " 'mask': 12808,\n",
       " 'obscuring': 14362,\n",
       " 'acting': 220,\n",
       " 'aggressively': 443,\n",
       " 'towards': 21366,\n",
       " 'decided': 5243,\n",
       " 'inform': 10589,\n",
       " 'proper': 16257,\n",
       " 'authorities': 1417,\n",
       " 'conference': 4243,\n",
       " 'advised': 360,\n",
       " 'loitering': 12320,\n",
       " 'line': 12185,\n",
       " 'scrimmage': 18417,\n",
       " 'sensitive': 18607,\n",
       " 'areas': 1080,\n",
       " 'avoid': 1468,\n",
       " 'similar': 19064,\n",
       " 'incidents': 10416,\n",
       " 'moving': 13620,\n",
       " 'forward': 8288,\n",
       " 'described': 5586,\n",
       " 'unidentified': 21990,\n",
       " 'object': 14340,\n",
       " 'hands': 9403,\n",
       " 'description': 5589,\n",
       " 'prompted': 16238,\n",
       " 'officials': 14449,\n",
       " 'detain': 5647,\n",
       " 'perform': 15271,\n",
       " 'thorough': 21079,\n",
       " 'strip': 20173,\n",
       " 'search': 18458,\n",
       " 'relieved': 17232,\n",
       " 'discover': 5939,\n",
       " 'football': 8191,\n",
       " 'single': 19098,\n",
       " 'player': 15607,\n",
       " 'code': 3871,\n",
       " 'conduct': 4238,\n",
       " 'teammates': 20824,\n",
       " 'currently': 4984,\n",
       " 'held': 9639,\n",
       " 'questioning': 16596,\n",
       " 'suspicion': 20522,\n",
       " 'gang': 8592,\n",
       " 'related': 17200,\n",
       " 'activity': 230,\n",
       " 'eyewitnesses': 7524,\n",
       " 'observed': 14367,\n",
       " 'wearing': 22828,\n",
       " 'clothes': 3807,\n",
       " 'bearing': 1850,\n",
       " 'colors': 3965,\n",
       " 'threatening': 21093,\n",
       " 'logo': 12315,\n",
       " 'quashing': 16578,\n",
       " 'rumors': 17980,\n",
       " 'team': 20823,\n",
       " 'early': 6506,\n",
       " 'exit': 7363,\n",
       " 'las': 11830,\n",
       " 'vegas': 22349,\n",
       " 'oakland': 14322,\n",
       " 'raiders': 16687,\n",
       " 'announced': 866,\n",
       " 'entirety': 7000,\n",
       " 'home': 9865,\n",
       " 'schedule': 18311,\n",
       " 'head': 9546,\n",
       " 'coach': 3837,\n",
       " 'jon': 11239,\n",
       " 'gruden': 9177,\n",
       " 'backyard': 1573,\n",
       " 'really': 16886,\n",
       " 'perfect': 15269,\n",
       " 'venue': 22383,\n",
       " 'fact': 7552,\n",
       " 'playing': 15611,\n",
       " 'yard': 23294,\n",
       " 'nowhere': 14254,\n",
       " 'else': 6710,\n",
       " 'league': 11921,\n",
       " 'proposed': 16272,\n",
       " 'half': 9340,\n",
       " 'acre': 213,\n",
       " 'plot': 15646,\n",
       " 'nestled': 13991,\n",
       " 'bay': 1823,\n",
       " 'area': 1079,\n",
       " 'suburbs': 20295,\n",
       " 'boasted': 2309,\n",
       " 'natural': 13890,\n",
       " 'surface': 20467,\n",
       " 'enough': 6950,\n",
       " 'improvised': 10380,\n",
       " 'seating': 18475,\n",
       " 'accommodate': 137,\n",
       " 'dozens': 6311,\n",
       " 'hardcore': 9440,\n",
       " 'faithful': 7597,\n",
       " 'mistake': 13349,\n",
       " 'rocking': 17814,\n",
       " 'mean': 12943,\n",
       " 'derek': 5571,\n",
       " 'carr': 3118,\n",
       " 'delivering': 5414,\n",
       " 'strikes': 20168,\n",
       " 'deck': 5254,\n",
       " 'plus': 15667,\n",
       " 'fans': 7641,\n",
       " 'love': 12394,\n",
       " 'amenities': 720,\n",
       " 'room': 17867,\n",
       " 'black': 2172,\n",
       " 'hole': 9842,\n",
       " 'garbage': 8603,\n",
       " 'cans': 3027,\n",
       " 'got': 8977,\n",
       " 'bathrooms': 1799,\n",
       " 'crockpot': 4855,\n",
       " 'full': 8469,\n",
       " 'chili': 3516,\n",
       " 'house': 9993,\n",
       " 'better': 2047,\n",
       " 'spend': 19651,\n",
       " 'several': 18702,\n",
       " 'admitted': 306,\n",
       " 'despite': 5619,\n",
       " 'treacherous': 21511,\n",
       " 'clothesline': 3808,\n",
       " 'exposed': 7445,\n",
       " 'tree': 21529,\n",
       " 'roots': 17875,\n",
       " 'far': 7645,\n",
       " 'preferable': 15961,\n",
       " 'games': 8588,\n",
       " 'goddamn': 8910,\n",
       " 'baseball': 1767,\n",
       " 'humane': 10049,\n",
       " 'deal': 5194,\n",
       " 'suffering': 20334,\n",
       " 'cleveland': 3748,\n",
       " 'browns': 2691,\n",
       " 'tuesday': 21669,\n",
       " 'euthanized': 7212,\n",
       " 'dawg': 5166,\n",
       " 'pound': 15881,\n",
       " 'rabies': 16642,\n",
       " 'outbreak': 14714,\n",
       " 'part': 15042,\n",
       " 'heartbroken': 9583,\n",
       " 'cutting': 5020,\n",
       " 'lives': 12259,\n",
       " 'short': 18914,\n",
       " 'putting': 16513,\n",
       " 'option': 14612,\n",
       " 'owner': 14844,\n",
       " 'jimmy': 11188,\n",
       " 'haslam': 9493,\n",
       " 'revealed': 17592,\n",
       " 'concern': 4200,\n",
       " 'piqued': 15525,\n",
       " 'began': 1897,\n",
       " 'chewing': 3493,\n",
       " 'plastic': 15594,\n",
       " 'seats': 18476,\n",
       " 'salivating': 18119,\n",
       " 'uncontrollably': 21864,\n",
       " 'discovered': 5940,\n",
       " 'late': 11841,\n",
       " 'cure': 4974,\n",
       " 'administered': 292,\n",
       " 'put': 16499,\n",
       " 'seemed': 18532,\n",
       " 'fun': 8477,\n",
       " 'approachable': 1016,\n",
       " 'getting': 8766,\n",
       " 'aggressive': 442,\n",
       " 'bit': 2153,\n",
       " 'seem': 18531,\n",
       " 'quality': 16563,\n",
       " 'life': 12126,\n",
       " 'never': 14020,\n",
       " 'battling': 1818,\n",
       " 'constant': 4374,\n",
       " 'seizures': 18551,\n",
       " 'hydrophobia': 10122,\n",
       " 'resulting': 17532,\n",
       " 'making': 12599,\n",
       " 'impossible': 10355,\n",
       " 'drink': 6368,\n",
       " 'beer': 1887,\n",
       " 'emphasized': 6799,\n",
       " 'sadness': 18066,\n",
       " 'mercy': 13078,\n",
       " 'killing': 11535,\n",
       " 'assured': 1283,\n",
       " 'comfort': 3997,\n",
       " 'knowing': 11620,\n",
       " 'suffer': 20332,\n",
       " 'recognition': 16967,\n",
       " 'brave': 2544,\n",
       " 'altruistic': 679,\n",
       " 'risk': 17747,\n",
       " 'health': 9569,\n",
       " 'greater': 9081,\n",
       " 'good': 8948,\n",
       " 'pentagon': 15248,\n",
       " 'thursday': 21133,\n",
       " 'honor': 9900,\n",
       " 'sacrifices': 18052,\n",
       " 'jerseys': 11156,\n",
       " 'throughout': 21112,\n",
       " 'december': 5235,\n",
       " 'every': 7239,\n",
       " 'week': 22854,\n",
       " 'men': 13048,\n",
       " 'gridiron': 9116,\n",
       " 'bodies': 2320,\n",
       " 'soldiers': 19436,\n",
       " 'wear': 22826,\n",
       " 'caps': 3059,\n",
       " 'show': 18939,\n",
       " 'support': 20443,\n",
       " 'spokesperson': 19717,\n",
       " 'amato': 692,\n",
       " 'active': 223,\n",
       " 'duty': 6473,\n",
       " 'sporting': 19730,\n",
       " 'gear': 8664,\n",
       " 'teams': 20825,\n",
       " 'raise': 16700,\n",
       " 'awareness': 1489,\n",
       " 'people': 15251,\n",
       " 'aside': 1203,\n",
       " 'preserve': 16010,\n",
       " 'families': 7624,\n",
       " 'travel': 21500,\n",
       " 'cities': 3647,\n",
       " 'across': 217,\n",
       " 'uphold': 22173,\n",
       " 'nation': 13869,\n",
       " 'traditions': 21409,\n",
       " 'battered': 1808,\n",
       " 'bruised': 2696,\n",
       " 'years': 23305,\n",
       " 'often': 14459,\n",
       " 'cut': 5014,\n",
       " 'sit': 19126,\n",
       " 'barracks': 1739,\n",
       " 'enjoy': 6929,\n",
       " 'freedom': 8360,\n",
       " 'end': 6862,\n",
       " 'service': 18671,\n",
       " 'hopefully': 9926,\n",
       " 'shows': 18949,\n",
       " 'officers': 14443,\n",
       " 'true': 21630,\n",
       " 'heroes': 9701,\n",
       " 'welling': 22888,\n",
       " 'emotion': 6786,\n",
       " 'upon': 22182,\n",
       " 'finally': 7929,\n",
       " 'setting': 18684,\n",
       " 'foot': 8189,\n",
       " 'hallowed': 9347,\n",
       " 'tile': 21171,\n",
       " 'college': 3941,\n",
       " 'senior': 18594,\n",
       " 'anthony': 901,\n",
       " 'harper': 9471,\n",
       " 'fulfilled': 8465,\n",
       " 'lifelong': 12131,\n",
       " 'dream': 6347,\n",
       " 'saturday': 18231,\n",
       " 'allowed': 626,\n",
       " 'shower': 18945,\n",
       " 'notre': 14232,\n",
       " 'dame': 5079,\n",
       " 'showers': 18946,\n",
       " 'knew': 11604,\n",
       " 'worked': 23168,\n",
       " 'quit': 16622,\n",
       " 'takes': 20699,\n",
       " 'lather': 11845,\n",
       " 'ovation': 14765,\n",
       " 'brian': 2603,\n",
       " 'kelly': 11452,\n",
       " 'tossed': 21329,\n",
       " 'conditioner': 4234,\n",
       " 'bench': 1969,\n",
       " 'locker': 12297,\n",
       " 'watch': 22784,\n",
       " 'wishing': 23095,\n",
       " 'always': 684,\n",
       " 'thought': 21083,\n",
       " 'soap': 19385,\n",
       " 'goes': 8919,\n",
       " 'grit': 9144,\n",
       " 'determination': 5668,\n",
       " 'anyone': 930,\n",
       " 'achieve': 186,\n",
       " 'bathe': 1794,\n",
       " 'entire': 6998,\n",
       " 'witness': 23115,\n",
       " 'announcement': 867,\n",
       " 'perceived': 15260,\n",
       " 'major': 12588,\n",
       " 'reassurance': 16908,\n",
       " 'parents': 15014,\n",
       " 'children': 3514,\n",
       " 'low': 12401,\n",
       " 'cognitive': 3889,\n",
       " 'abilities': 49,\n",
       " 'subpar': 20261,\n",
       " 'reasoning': 16903,\n",
       " 'pediatric': 15199,\n",
       " 'experts': 7404,\n",
       " 'report': 17360,\n",
       " 'claims': 3667,\n",
       " 'contact': 4413,\n",
       " 'poses': 15826,\n",
       " 'little': 12247,\n",
       " 'brains': 2521,\n",
       " 'already': 656,\n",
       " 'well': 22884,\n",
       " 'tackle': 20664,\n",
       " 'long': 12332,\n",
       " 'known': 11623,\n",
       " 'high': 9743,\n",
       " 'sport': 19728,\n",
       " 'particularly': 15052,\n",
       " 'poor': 15772,\n",
       " 'guys': 9281,\n",
       " 'knuckle': 11625,\n",
       " 'draggers': 6321,\n",
       " 'away': 1490,\n",
       " 'lose': 12368,\n",
       " 'university': 22029,\n",
       " 'chicago': 3497,\n",
       " 'childhood': 3512,\n",
       " 'development': 5702,\n",
       " 'expert': 7403,\n",
       " 'dr': 6314,\n",
       " 'maureen': 12884,\n",
       " 'clifford': 3759,\n",
       " 'neuropathological': 14007,\n",
       " 'research': 17420,\n",
       " 'led': 11954,\n",
       " 'conclusion': 4215,\n",
       " 'chronic': 3592,\n",
       " 'traumatic': 21498,\n",
       " 'encephalopathy': 6840,\n",
       " 'caused': 3226,\n",
       " 'repeated': 17336,\n",
       " 'severe': 18703,\n",
       " 'impacts': 10297,\n",
       " 'mitigated': 13363,\n",
       " 'percent': 15261,\n",
       " 'cases': 3154,\n",
       " 'youth': 23365,\n",
       " 'presented': 16006,\n",
       " 'signs': 19045,\n",
       " 'huge': 10037,\n",
       " 'dumbass': 6437,\n",
       " 'clearly': 3737,\n",
       " 'couple': 4680,\n",
       " 'screws': 18414,\n",
       " 'loose': 12350,\n",
       " 'course': 4692,\n",
       " 'cte': 4928,\n",
       " 'danger': 5099,\n",
       " 'comes': 3996,\n",
       " 'sports': 19731,\n",
       " 'ages': 435,\n",
       " 'crucial': 4886,\n",
       " 'healthy': 9571,\n",
       " 'neurological': 14005,\n",
       " 'growth': 9175,\n",
       " 'symptoms': 20624,\n",
       " 'mood': 13515,\n",
       " 'swings': 20587,\n",
       " 'difficult': 5801,\n",
       " 'thinking': 21056,\n",
       " 'memory': 13047,\n",
       " 'sounds': 19517,\n",
       " 'kid': 11515,\n",
       " 'precious': 15935,\n",
       " 'dude': 6425,\n",
       " 'bonehead': 2379,\n",
       " 'blocking': 2256,\n",
       " 'tackling': 20665,\n",
       " 'hit': 9806,\n",
       " 'crossing': 4871,\n",
       " 'routes': 17915,\n",
       " 'reasons': 16904,\n",
       " 'idiot': 10188,\n",
       " 'study': 20212,\n",
       " 'concluded': 4213,\n",
       " 'halfwits': 9342,\n",
       " 'shot': 18927,\n",
       " 'success': 20302,\n",
       " 'staring': 19891,\n",
       " 'wide': 22996,\n",
       " 'eyed': 7517,\n",
       " 'table': 20652,\n",
       " 'unopened': 22060,\n",
       " 'presents': 16009,\n",
       " 'largely': 11823,\n",
       " 'ignored': 10214,\n",
       " 'guests': 9226,\n",
       " 'local': 12288,\n",
       " 'rick': 17683,\n",
       " 'joseph': 11252,\n",
       " 'reportedly': 17363,\n",
       " 'watched': 22786,\n",
       " 'helplessly': 9660,\n",
       " 'white': 22967,\n",
       " 'elephant': 6679,\n",
       " 'exchange': 7299,\n",
       " 'devolved': 5721,\n",
       " 'friends': 8395,\n",
       " 'chatting': 3437,\n",
       " 'nice': 14059,\n",
       " 'christ': 3579,\n",
       " 'turn': 21710,\n",
       " 'pick': 15448,\n",
       " 'ago': 450,\n",
       " 'derailed': 5568,\n",
       " 'everyone': 7242,\n",
       " 'blabbing': 2171,\n",
       " 'fucking': 8451,\n",
       " 'christmas': 3588,\n",
       " 'forced': 8207,\n",
       " 'listen': 12225,\n",
       " 'engaged': 6901,\n",
       " 'pleasant': 15620,\n",
       " 'conversations': 4513,\n",
       " 'favorite': 7722,\n",
       " 'recipes': 16947,\n",
       " 'beloved': 1961,\n",
       " 'memories': 13045,\n",
       " 'festive': 7843,\n",
       " 'season': 18468,\n",
       " 'ugh': 21791,\n",
       " 'disaster': 5906,\n",
       " 'chumps': 3597,\n",
       " 'strategizing': 20123,\n",
       " 'screw': 18412,\n",
       " 'best': 2030,\n",
       " 'instead': 10729,\n",
       " ...}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_vec.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `HashingVectorizer`\n",
    "\n",
    "There is also a hashing vectorizer, which will encrypt all the words of the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1048566</th>\n",
       "      <th>1048567</th>\n",
       "      <th>1048568</th>\n",
       "      <th>1048569</th>\n",
       "      <th>1048570</th>\n",
       "      <th>1048571</th>\n",
       "      <th>1048572</th>\n",
       "      <th>1048573</th>\n",
       "      <th>1048574</th>\n",
       "      <th>1048575</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.167248</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 1048576 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0        1        2        3        4        5        6        7        \\\n",
       "0    0.000000      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "1    0.167248      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "2    0.000000      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "3    0.000000      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "4    0.000000      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "..        ...      ...      ...      ...      ...      ...      ...      ...   \n",
       "995  0.000000      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "996  0.000000      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "997  0.000000      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "998  0.000000      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "999  0.000000      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "\n",
       "     8        9        ...  1048566  1048567  1048568  1048569  1048570  \\\n",
       "0        0.0      0.0  ...      0.0      0.0      0.0      0.0      0.0   \n",
       "1        0.0      0.0  ...      0.0      0.0      0.0      0.0      0.0   \n",
       "2        0.0      0.0  ...      0.0      0.0      0.0      0.0      0.0   \n",
       "3        0.0      0.0  ...      0.0      0.0      0.0      0.0      0.0   \n",
       "4        0.0      0.0  ...      0.0      0.0      0.0      0.0      0.0   \n",
       "..       ...      ...  ...      ...      ...      ...      ...      ...   \n",
       "995      0.0      0.0  ...      0.0      0.0      0.0      0.0      0.0   \n",
       "996      0.0      0.0  ...      0.0      0.0      0.0      0.0      0.0   \n",
       "997      0.0      0.0  ...      0.0      0.0      0.0      0.0      0.0   \n",
       "998      0.0      0.0  ...      0.0      0.0      0.0      0.0      0.0   \n",
       "999      0.0      0.0  ...      0.0      0.0      0.0      0.0      0.0   \n",
       "\n",
       "     1048571  1048572  1048573  1048574  1048575  \n",
       "0        0.0      0.0      0.0      0.0      0.0  \n",
       "1        0.0      0.0      0.0      0.0      0.0  \n",
       "2        0.0      0.0      0.0      0.0      0.0  \n",
       "3        0.0      0.0      0.0      0.0      0.0  \n",
       "4        0.0      0.0      0.0      0.0      0.0  \n",
       "..       ...      ...      ...      ...      ...  \n",
       "995      0.0      0.0      0.0      0.0      0.0  \n",
       "996      0.0      0.0      0.0      0.0      0.0  \n",
       "997      0.0      0.0      0.0      0.0      0.0  \n",
       "998      0.0      0.0      0.0      0.0      0.0  \n",
       "999      0.0      0.0      0.0      0.0      0.0  \n",
       "\n",
       "[1000 rows x 1048576 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hvec = HashingVectorizer(token_pattern=r\"([a-zA-Z]+(?:'[a-z]+)?)\",\n",
    "                         stop_words=sw)\n",
    "X = hvec.fit_transform(corpus.body)\n",
    "\n",
    "df_cv = pd.DataFrame(X.toarray())\n",
    "df_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some rules of thumb about these vectorizers:\n",
    "\n",
    "**Tf-Idf**: Probably the most commonly used. Useful when the goal is to distinguish the **content** of documents from others in the corpus.\n",
    "\n",
    "**Count**: Useful when the words themselves matter. If the goal is instead about identifying authors by their words, then the fact that some word appears in many documents of the corpus may be important.\n",
    "\n",
    "**Hashing**: The advantage here is speed and low memory usage. The disadvantage is that you lose the identities of the words being tokenized. Useful for very large datasets where the ultimate model may be a bit of a black box."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "For a final exercise, work through the following:\n",
    "\n",
    "Create a document term matrix of the 1000-document corpus. The vocabulary should have no stopwords, numbers, or punctuation, and it should be lemmatized. Use a `TfidfVectorizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Answer</summary>\n",
    "    <code># Tokenizing\n",
    "tokenized_docs = [tokenizer.tokenize(doc) for doc in corpus['body']]\n",
    "lower_docs = [[token.lower() for token in doc] for doc in tokenized_docs]\n",
    "sw_docs = [[token for token in doc if token not in sw] for doc in lower_docs]\n",
    "# Initial tagging\n",
    "docs_tagged = [pos_tag(doc) for doc in sw_docs]\n",
    "# Tag with Wordnet tags\n",
    "wordnet_docs_tagged = [[(token[0], get_wordnet_pos(token[1]))\n",
    "             for token in doc] for doc in docs_tagged]\n",
    "# Lemmatize\n",
    "docs_lemmed = [[lemmatizer.lemmatize(token[0], token[1]) for token in doc]\\\n",
    "               for doc in wordnet_docs_tagged]\n",
    "# Use the tf-idf vectorizer to create the matrix\n",
    "X = tf_vec.fit_transform([' '.join(doc) for doc in docs_lemmed])\n",
    "df = pd.DataFrame(X.toarray(), columns=tf_vec.get_feature_names())</code>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
